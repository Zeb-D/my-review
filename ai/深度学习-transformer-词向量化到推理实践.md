本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------

### 背景

作为词向量化到推理的系列中，本篇作为本地优化篇。在上篇[深度学习-RNN-词向量化到推理实践.md](.深度学习-RNN-词向量化到推理实践.md) 的rnn模型存在较多的提升空间，这里主要分为训练质量、推理质量这两个大维度，所以这次文章接着transformer模型来接着“为自己写歌”。



我们先普及下transformer基本概念和原理



### 概念

#### 基本概念

1. 起源与定位
   - Transformer 是 2017 年由 Google 在论文《Attention Is All You Need》中提出的深度学习架构，最初用于自然语言处理（NLP）任务（如机器翻译），现已广泛应用于计算机视觉（CV）、语音处理等领域。
   - 区别于传统循环神经网络（RNN）和卷积神经网络（CNN），Transformer 完全基于**自注意力机制（Self-Attention）**，实现了并行计算和长距离依赖建模，大幅提升了训练效率和性能。
2. 核心用途
   - 解决 RNN 无法并行处理长序列的问题（如 LSTM/GRU 受限于时序依赖）。
   - 替代 CNN 的多层卷积操作，通过注意力机制直接捕捉全局语义关联。



#### 工作原理

Transformer 的核心架构由**编码器（Encoder）** 和**解码器（Decoder）** 组成，两者均由多个相同模块堆叠而成（通常为 6 层）。

##### 编码器结构与工作流程

1. 模块组成

   - 每个编码器模块包含：
     - **多头自注意力层（Multi-Head Self-Attention）**：捕捉序列内部各位置的依赖关系。
     - **前馈神经网络（Feed Forward Network）**：对注意力输出进行非线性变换。
     - **残差连接（Residual Connection）** 和**层归一化（Layer Normalization）**：稳定训练过程。

2. 工作流程

   - **输入处理**：输入序列（如单词嵌入向量）先与**位置编码（Positional Encoding）** 相加，获取时序信息。
   - 多头自注意力计算:
     1. 将输入映射为查询向量（Query, Q）、键向量（Key, K）和值向量（Value, V）。
     2. 计算任意两个位置的注意力分数： \(\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V\) 其中，\(\sqrt{d_k}\)为缩放因子，避免梯度消失。
     3. 多头机制将 Q、K、V 分割成多个子空间并行计算，再拼接结果，增强模型捕捉不同语义信息的能力。
   - **前馈网络**：对注意力输出进行线性变换和激活函数（如 ReLU）处理： \(\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2\)
   - **残差与归一化**：每一层输出为\(\text{LayerNorm}(x + \text{Attention}(x))\)，缓解梯度消失问题。

   

   ##### 解码器结构与工作流程

   1. 模块差异
      - 解码器在编码器基础上增加：
        - **掩码多头自注意力层（Masked Multi-Head Self-Attention）**：确保解码时只关注已生成的位置（避免未来信息泄露）。
        - **编码器 - 解码器注意力层（Encoder-Decoder Attention）**：解码器通过该层关注编码器的输出，获取源序列语义。
   2. 工作流程
      - **输入处理**：解码器输入为已生成的目标序列（如翻译后的单词），同样添加位置编码。
      - **掩码自注意力**：通过掩码矩阵屏蔽未来位置的注意力计算，保证自回归生成逻辑。
      - **编码器 - 解码器注意力**：解码器的 Q 来自当前层输入，K 和 V 来自编码器的最终输出，实现源序列与目标序列的语义对齐。
      - **输出层**：最后一层解码器的输出通过线性层和 softmax 函数，生成下一个 token 的概率分布。
