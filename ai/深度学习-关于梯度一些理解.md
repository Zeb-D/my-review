本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------

### 背景

看到书上一堆单词(名词) 如SGD-小批量随机梯度下降，Adam，这些都讲到了梯度这个概念。

众所周知，机器学习的最终目标就是找到一堆参数矩阵，在这个过程，需要优化算法来减少计算量、加快训练，从而找到一条拟合的函数曲线。

接下来主要通过小批量随机梯度下降（Mini-batch Stochastic Gradient Descent，简称小批量SGD）来展开对梯度二字的理解研究。





### 何为梯度

#### 梯度的数学定义

梯度（Gradient）是**多元函数的导数推广**，表示函数在某一点处**各个方向上的变化率**。对于一个标量函数 f(x)*f*(**x**)（其中 x=[x1,x2,...,xn]**x**=[*x*1,*x*2,...,*x**n*]），其梯度是一个向量：
$$
\nabla f(\mathbf{x}) = \left[ \frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2}, ..., \frac{\partial f}{\partial x_n} \right]
$$
**物理意义**：梯度指向函数值增长最快的方向，其大小表示变化速率。



#### 梯度在机器学习中的作用

梯度是优化算法的核心工具，主要作用包括：

##### (1) 指导参数更新

- 在训练模型时，通过**梯度下降法**（如SGD、Adam）沿梯度反方向调整参数，最小化损失函数：
  $$
  W_{t+1} = W_t - \eta \cdot \nabla_w J(W)
  $$
  

  - w**w**：模型参数（如权重）
  - η：学习率（步长）
  - *J*(**w**)：损失函数

##### (2) 衡量参数重要性

- 梯度的幅值反映了参数对损失函数的敏感度：
  - 大梯度 → 参数轻微变化会显著影响输出 → 需谨慎调整
  - 小梯度 → 参数对输出影响小 → 可大幅调整

##### (3) 诊断模型问题

- **梯度消失**：梯度接近零，导致参数无法更新（常见于深层网络）。
- **梯度爆炸**：梯度极大，导致参数更新不稳定（可通过梯度裁剪解决）。



#### 小批量随机梯度下降

小批量随机梯度下降（Mini-batch Stochastic Gradient Descent，简称小批量SGD）是机器学习和深度学习中**最常用的优化算法**，它结合了批量梯度下降和随机梯度下降的优点。

以下是其核心要点：

##### 基本思想

- **批量梯度下降（BGD）**：使用全部数据计算梯度，计算精确但速度慢。
- **随机梯度下降（SGD）**：每次随机用1个样本计算梯度，速度快但波动大。
- **小批量SGD**：折中方案，每次随机选取**一小批样本（batch）**计算梯度，平衡了效率和稳定性。

##### 算法步骤

对于每个训练迭代（iteration）：

1. **随机采样**：从数据集中随机抽取一个小批量（例如32、64个样本）。

2. **计算梯度**：用该批数据计算损失函数关于参数的梯度。

3. **更新参数**：沿梯度反方向调整参数，步长由学习率（`lr`）控制：
   $$
   \theta_{t+1} = \theta_t - \eta \cdot \nabla_\theta J(\theta)
   $$
   

其中：

- θ*θ*：模型参数（如权重w*w*、偏置b*b*）
- η*η*：学习率
- ∇θJ：当前批次的梯度

##### 为什么使用小批量SGD

- **内存效率**：避免一次性加载全部数据（适合GPU内存限制）。
- **更快收敛**：比BGD更少的迭代次数即可接近最优解。
- **隐式正则化**：梯度噪声可能提升模型泛化能力。

##### 超参数选择建议

- **批量大小**：通常32-256，需权衡速度和稳定性。
- **学习率**：常用0.01或0.001，可通过学习率调度器动态调整。
- **迭代次数（epochs）**：监控验证集损失决定早停。

小批量SGD是深度学习优化的基石，后续改进算法（如Adam、RMSProp）均以其为基础。



### 实际例子

#### 线性回归中的梯度

假设损失函数为均方误差（MSE）：
$$
J(w, b) = \frac{1}{2m} \sum_{i=1}^m (y_i - (w x_i + b))^2
$$
其梯度为：
$$
\nabla_w J = -\frac{1}{m} \sum_{i=1}^m (y_i - (w x_i + b)) x_i
\newline
\nabla_b J = -\frac{1}{m} \sum_{i=1}^m (y_i - (w x_i + b))
$$
通过梯度下降更新参数：
$$
w \leftarrow w - \eta \cdot \nabla_w J
\newline
b \leftarrow b - \eta \cdot \nabla_b J
$$


#### 代码示例

```
if __name__ == "__main__":
    import torch

    # 定义模型和损失函数
    w = torch.tensor([1.0], requires_grad=True)  # 初始权重 w=1.0
    b = torch.tensor([0.0], requires_grad=True)  # 初始偏置 b=0.0
    X = torch.tensor([2.0])  # 输入特征 X=2.0
    y_true = torch.tensor([3.0])  # 真实标签 y_true=3.0

    # 前向计算
    y_pred = w * X + b  # y_pred = 1.0 * 2.0 + 0.0 = 2.0
    loss = (y_pred - y_true) ** 2  # loss = (2.0 - 3.0)^2 = 1.0
    print(f'y_pred: {y_pred}')
    print(f'loss: {loss}')

    # 反向传播计算梯度
    loss.backward()

    print("梯度 w:", w.grad)  # 输出: tensor([-4.])
    print("梯度 b:", b.grad)  # 输出: tensor([-2.])
```

关于前向计算的计算图如下：

```
w (1.0)   X (2.0)
    \     /
    乘法 → (2.0)
      \
      加法 → (2.0)   y_true (3.0)
         \         /
          减法 → (-1.0)
             \
              平方 → loss (1.0)
```



梯度计算过程：

梯度指向损失函数增长最快的方向，而优化时需要沿**梯度反方向**更新参数（即负梯度方向）。

`loss.backward()`反向传播计算过程：

对于损失函数：
$$
\text{loss} = (y_\text{pred} - y_\text{true})^2 = (w \cdot X + b - y_\text{true})^2
$$
梯度计算公式：
$$
\frac{\partial \text{loss}}{\partial w} = 2 \cdot (w \cdot X + b - y_\text{true}) \cdot X
\newline
\frac{\partial \text{loss}}{\partial b} = 2 \cdot (w \cdot X + b - y_\text{true})
$$
代入具体值：
$$
\frac{\partial \text{loss}}{\partial w} = 2 \cdot (1 \cdot 2 + 0 - 3) \cdot 2 = -4
\newline
\frac{\partial \text{loss}}{\partial b} = 2 \cdot (1 \cdot 2 + 0 - 3) = -2
$$


#### 关于梯度启用

在PyTorch中，`loss`与`w`和`b`的关联是通过**计算图（Computational Graph）**自动建立的，即使它们在代码中没有直接的函数调用关系。只有设为`True`的张量才会参与梯度计算。

过程如下：

1. 计算图的隐式构建

当执行以下操作时：

```
w = torch.tensor([1.0], requires_grad=True)  # 启用梯度跟踪
b = torch.tensor([0.0], requires_grad=True)
y_pred = w * X + b       # 构建计算图：乘法→加法
loss = (y_pred - y_true) ** 2  # 构建计算图：减法→平方
```

PyTorch会**自动记录运算过程**，生成如下计算图：

```
w (叶子节点)   X (输入数据)
     \       /
     乘法 → 中间结果
        \
         加法 → y_pred (依赖w和b)
           \ 
            减法 → 中间结果
              \
               平方 → loss (最终标量)
```

- **关键点**：所有`requires_grad=True`的张量（如`w`、`b`）会被自动加入计算图。



反向传播时的梯度计算

当调用`loss.backward()`时：

1. **从loss开始反向传播**：根据链式法则，计算`loss`对所有叶子节点（`w`、`b`）的梯度。
2. **具体路径**：
   - `loss → (y_pred - y_true)^2 → y_pred → w*X + b → w`
   - `loss → (y_pred - y_true)^2 → y_pred → w*X + b → b`

数学推导：
$$
\frac{\partial \text{loss}}{\partial w} = \frac{\partial \text{loss}}{\partial \text{y\_pred}} \cdot \frac{\partial \text{y\_pred}}{\partial w} = 2(y_\text{pred}-y_\text{true}) \cdot X
\newline
\frac{\partial \text{loss}}{\partial b} = \frac{\partial \text{loss}}{\partial \text{y\_pred}} \cdot \frac{\partial \text{y\_pred}}{\partial b} = 2(y_\text{pred}-y_\text{true})
$$




梯度结果的存储

- **`w.grad`和`b.grad`**：PyTorch会将计算出的梯度存储在张量的`.grad`属性中。
- **示例中的数值**：
  - 代入`w=1.0`, `b=0.0`, `X=2.0`, `y_true=3.0`：
    - `y_pred = 1.0*2.0 + 0.0 = 2.0`
    - `∂loss/∂w = 2*(2.0-3.0)*2.0 = -4.0`
    - `∂loss/∂b = 2*(2.0-3.0) = -2.0`



#### 梯度在矩阵的列子

在矩阵或者张量(tensor)中，涉及运算符号@ 即向量积。

```
if __name__ == "__main__":
    import torch
    
    print("矩阵的梯度")
    # 定义多元素参数（d=3个特征，n=4个样本）
    w = torch.tensor([[2.0], [-1.0], [3.0]], requires_grad=True)  # shape (3, 1)
    X = torch.tensor([[1.0, 2.0, 3.0],
                      [4.0, 5.0, 6.0],
                      [7.0, 8.0, 9.0],
                      [10.0, 11.0, 12.0]])  # shape (4, 3)：4个样本、3个特征
    b = torch.tensor(1.0, requires_grad=True)  # 标量 1.0
    y_pred = X @ w + b  # 矩阵乘法 (@) + 广播加法
    # y_pred shape: (4, 1)

    y_true = torch.tensor([[7.0], [13.0], [19.0], [25.0]])  # 特征： shape (4, 1)
    loss = torch.mean((y_pred - y_true) ** 2)  # MSE损失：均方差

    loss.backward()  # 自动计算梯度
    print("梯度 w:\n", w.grad)  # shape (3, 1)
    print("梯度 b:", b.grad)  # 标量

    n = X.shape[0]
    residual = y_pred - y_true  # shape (4, 1)

    # 手动计算梯度
    manual_w_grad = (2 / n) * X.T @ residual  # X^T shape (3,4), residual shape (4,1)
    manual_b_grad = (2 / n) * torch.sum(residual)
    print("手动计算 n:", n)
    print("手动计算 w梯度:\n", manual_w_grad)
    print("手动计算 b梯度:", manual_b_grad)

```

输出：

```
矩阵的梯度
梯度 w:
 tensor([[177.],
        [201.],
        [225.]])
梯度 b: tensor(24.)
手动计算 n: 4
手动计算 w梯度:
 tensor([[177.],
        [201.],
        [225.]], grad_fn=<MmBackward0>)
手动计算 b梯度: tensor(24., grad_fn=<MulBackward0>)
```



计算过程如下：

**梯度计算的数学推导**

对于 MSE 损失：
$$
\text{MSE} = \frac{1}{n}\sum_{i=1}^n (y\_pred_i - y\_true_i)^2
$$
(1) 对权重 `w` 的梯度
$$
\frac{\partial \text{MSE}}{\partial \mathbf{w}} = \frac{2}{n} \mathbf{X}^\top (\mathbf{y\_pred} - \mathbf{y\_true})
$$


- 结果形状：`(d, 1)`（与 `w` 相同）

(2) 对偏置 `b` 的梯度
$$
\frac{\partial \text{MSE}}{\partial b} = \frac{2}{n} \sum_{i=1}^n (y\_pred_i - y\_true_i)
$$


- 结果形状：标量（与 `b` 相同）
