本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------

### 背景

如果你想大模型领域入门开发，包括训练、微调、推理，这些都是和python环境强相关的。如果是MCP、agent等大模型应用开发，那么还可以有其他语言(比如Java、golang)，这是它们只是调用了部署完的大模型api，用行业内的话来说，面向OpenAPI编程。



那本文的内容是搭建初级的python环境，有了环境后面介绍的大模型领域就能进入到实战之中。



### 环境安装

现在很多大模型相关的开源框架对python环境要求很敏感即你可能换个小版本，那么你对应的依赖库运行不了。

Python 分 python2 和python3 这两大版本，现在的大模型经常用到的环境是在python3.9 及以上。



大多数环境一般用conanda 或者pyenv进行隔离。



#### Conanda 安装及使用

##### 安装 Conanda

1. **下载 Miniconda 安装包**
   打开终端，执行以下命令下载 Miniconda（轻量级 Conanda）：

   ```bash
   curl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh
   ```

2. **执行安装脚本**

   ```bash
   bash Miniconda3-latest-MacOSX-x86_64.sh
   ```

   安装过程中按提示操作，安装完成后重启终端。

3. **验证安装**

   ```bash
   conda --version
   ```

​	为 Conanda 添加清华源：

```bash
conda config --add channels https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/free/
conda config --set show_channel_urls yes
```



##### 基本使用

1. **创建虚拟环境**

   ```bash
   conda create --name myenv python=3.8
   ```

2. **激活环境**

   ```bash
   conda activate myenv
   ```

3. **安装包**

   ```bash
   conda install numpy pandas
   ```

4. **查看已安装包**

   ```bash
   conda list
   ```

5. **退出环境**

   ```bash
   conda deactivate
   ```

6. **删除环境**

   ```bash
   conda remove --name myenv --all
   ```

   

#### Pyenv 安装及使用

##### 安装 Pyenv

1. 安装 Pyenv：

   ```bash
   brew install pyenv
   ```

2. **配置环境变量**
   将以下内容添加到`~/.zshrc`（或`~/.bashrc`）：

   ```bash
   echo 'export PYENV_ROOT="$HOME/.pyenv"' >> ~/.zshrc
   echo 'command -v pyenv >/dev/null || export PATH="$PYENV_ROOT/bin:$PATH"' >> ~/.zshrc
   echo 'eval "$(pyenv init -)"' >> ~/.zshrc
   ```

   重启终端或执行：

   ```bash
   source ~/.zshrc
   ```

3. **验证安装**

   ```bash
   pyenv --version
   ```



##### 基本使用

1. **列出可用 Python 版本**

   ```bash
   pyenv install --list
   ```

2. **安装python指定版本**

   ```bash
   pyenv install 3.9.7
   ```

3. **查看已安装版本**

   ```bash
   pyenv versions
   ```

4. **设置全局 Python 版本**

   ```bash
   pyenv global 3.9.7
   ```

5. **为项目创建虚拟环境**

   ```bash
   pyenv virtualenv 3.9.7 myproject-env
   ```

6. **激活 / 停用虚拟环境**

   ```bash
   pyenv activate myproject-env
   pyenv deactivate
   ```

7. **删除虚拟环境**

   ```bash
   pyenv uninstall myproject-env
   ```

   

#### Conanda 与 Pyenv 对比

| **功能**       | **Conanda**                          | **Pyenv**                           |
| -------------- | ------------------------------------ | ----------------------------------- |
| **包管理**     | 支持 Python 包及系统依赖（如 NumPy） | 仅管理 Python 版本，需配合 pip 使用 |
| **虚拟环境**   | 直接创建隔离环境                     | 通过 pyenv-virtualenv 插件实现      |
| **跨平台**     | 支持 Windows、macOS、Linux           | 主要用于 macOS/Linux                |
| **安装复杂度** | 较高（需下载完整安装包）             | 较低（通过 Homebrew 安装）          |



#### 安装CUDA（可选）

如果大家有机会想用显卡的话，那么这节是有必要的。

如果你的是mac m系列，那么是安装不了的 即不支持。

下面是linux系统的安装方式：

1. 系统配置（以Ubuntu 22.04为例）

```
sudo apt update && sudo apt upgrade -y
sudo apt install -y build-essential git curl wget unzip
```

2. 安装CUDA（适配NVIDIA GPU）

```
wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-ubuntu2204.pin
sudo mv cuda-ubuntu2204.pin /etc/apt/preferences.d/cuda-repository-pin-600
wget https://developer.download.nvidia.com/compute/cuda/12.3.1/local_installers/cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb
sudo dpkg -i cuda-repo-ubuntu2204-12-3-local_12.3.1-545.23.08-1_amd64.deb
sudo cp /var/cuda-repo-ubuntu2204-12-3-local/cuda-*-keyring.gpg /usr/share/keyrings/
sudo apt update
sudo apt install -y cuda
```

3. 安装cuDNN（深度学习加速库）

```
从NVIDIA官网下载对应版本的cuDNN deb包并安装

sudo dpkg -i libcudnn8_*.deb
sudo dpkg -i libcudnn8-dev_*.deb
```

4. 配置环境变量

```
echo 'export PATH=/usr/local/cuda/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
```



### 本地运行大模型

本地用开发者角色体检大模型，有很多种本地运行方式，有的用docker启动部署，有的要求配置高些。

市场上比较常见的是Transformers运行 和低配版的**llama.cpp**



#### transformers示例

1. **创建新的虚拟环境**

   ```bash
   python3.9 -m venv transformers-clean
   source transformers-clean/bin/activate
   ```

2. **安装最小依赖集**

   ```bash
   pip install transformers==4.35.2 torch==2.1.0 gradio
   ```

代码：

```
# text_generation_optimized.py
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, logging


# 检查环境
def check_environment():
    # 检查PyTorch版本
    torch_version = tuple(map(int, torch.__version__.split('.')[:2]))
    if torch_version < (2, 1):
        print(f"警告: 检测到PyTorch版本 {torch.__version__}，建议升级到2.1.0或更高版本")
        print("使用PyTorch 1.12.0时可能需要手动指定模型参数类型")

    # 检查CUDA可用性
    if not torch.cuda.is_available():
        print("警告: CUDA不可用，将使用CPU进行推理，速度较慢")

    # 检查Transformers版本
    import transformers
    print(f"PyTorch版本: {torch.__version__}")
    print(f"Transformers版本: {transformers.__version__}")
    print(f"CUDA可用: {torch.cuda.is_available()}")


check_environment()

# 禁用不必要的警告
logging.set_verbosity_error()

# 模型ID
model_id = "gpt2"

# 加载分词器
try:
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    print(f"分词器加载成功: {model_id}")
except Exception as e:
    print(f"分词器加载失败: {e}")
    exit(1)

# 手动指定模型参数类型以兼容旧版PyTorch
try:
    # 尝试半精度加载（需GPU支持）
    if torch.cuda.is_available():
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float16,  # 半精度
            low_cpu_mem_usage=True
        )
    else:
        # CPU环境使用float32
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float32,  # 全精度
            low_cpu_mem_usage=True
        )
    print(f"模型加载成功: {model_id}")
except Exception as e:
    print(f"模型加载失败: {e}")
    print("尝试使用CPU和全精度重新加载...")
    try:
        # 备选方案：强制CPU和全精度
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            torch_dtype=torch.float32,
            device_map="cpu"
        )
        print(f"模型已使用CPU加载: {model_id}")
    except Exception as e2:
        print(f"最终加载失败: {e2}")
        print("请确保PyTorch正确安装或升级到最新版本")
        exit(1)


# 生成函数
def generate_text(prompt, max_length=100, temperature=0.7):
    # 编码输入
    inputs = tokenizer(prompt, return_tensors="pt")

    # 将输入移至模型设备
    if hasattr(model, 'device'):
        inputs = {k: v.to(model.device) for k, v in inputs.items()}

    # 生成文本
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_length=max_length,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

    # 解码输出
    return tokenizer.decode(outputs[0], skip_special_tokens=True)


# 测试
if __name__ == "__main__":
    prompt = "Once upon a time"
    print("\n生成文本:")
    print(generate_text(prompt, max_length=100))
```

日志：

```
/Users/lucas/.pyenv/versions/3.9.18/bin/python /Users/lucas/PycharmProjects/ai/transformers/Transformers_test.py 
警告: CUDA不可用，将使用CPU进行推理，速度较慢
PyTorch版本: 2.1.0
Transformers版本: 4.52.4
CUDA可用: False
分词器加载成功: gpt2
模型加载成功: gpt2

生成文本:
Once upon a time, I had the satisfaction, when you have the satisfaction, to see your soul, your soul, your soul, in the world.

I once said, 'What you do, you must do.' And you said, 'I will do nothing but do what you want.' And I thought, 'What a bad thing it is to think of nothing but the good.'"[Pg 556]

In this view, then, there is no doubt that the world

Process finished with exit code 0

```



#### llama.cpp 示例

