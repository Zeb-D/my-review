本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------

### 概念

循环神经网络（Recurrent Neural Network，RNN）是一种基于顺序或时间序列数据进行训练的深度神经网络，可根据序列输入做出序列预测或结论1。以下是关于它的详细介绍1：

#### 特点

- **记忆特性**：与传统神经网络不同，RNN 具有 “记忆” 功能，能从先前输入中获取信息来影响当前的输入和输出，其输出取决于序列中的先验元素。通过在每个时间步长维护隐藏状态来跟踪上下文，隐藏状态充当存储有关先前输入信息的记忆。
- **参数共享**：循环网络在网络的每一层共享相同的权重参数，而前馈网络在每个节点上具有不同的权重。这些权重在反向传播和梯度下降过程中进行调整，以促进强化学习。

### 工作原理

- **输入处理**：在每个时间步长，RNN 接收当前输入（如句子中的单词、时间序列中的数据点）以及来自前一个时间步长的隐藏状态，将两者结合进行处理。
- **激活函数**：应用激活函数于网络中每一层神经元的输出，引入非线性，使网络能够处理非线性问题，控制神经元输出幅度，防止值在传递过程中过大或过小。常见的激活函数有 Sigmoid 函数、Tanh（双曲正切）函数、ReLU（修正线性单元）及其变体等。
- **输出生成**：基于当前输入和隐藏状态，通过计算生成当前时间步长的输出，同时更新隐藏状态，传递到下一个时间步长，形成反馈循环。

#### 类型

- **标准版循环神经网络（RNN）**：每一时间步长的输出都取决于当前输入和上一时间步长的隐藏状态，擅长处理具有短期依赖性的简单任务，但存在梯度消失等问题，难以学习长期依赖关系。
- **双向循环神经网络（BRNN）**：单向 RNN 只能利用先前输入数据，而 BRNN 还可以拉取未来的数据，从而提高预测准确性。
- **长短期记忆（LSTM）**：为解决梯度消失和长期依赖问题而提出，在人工神经网络的隐藏层中设有 “单元”，包含输入门、输出门和遗忘门，通过控制信息流来更好地处理长期依赖关系。
- **门控循环单元（GRU）**：类似于 LSTM，可解决 RNN 模型的短期记忆问题。它不使用 “元胞状态” 调节信息，而是使用隐藏状态，通过重置门和更新门控制要保留的信息，架构更简洁，计算效率高，训练速度快，适用于实时或资源受限的应用程序。
- **编码器 - 解码器 RNN**：通常用于序列到序列任务，如机器翻译。编码器将输入序列处理为固定长度的向量（上下文），解码器则使用该上下文生成输出序列，但固定长度的上下文向量可能成为瓶颈，尤其是对于较长的输入序列。

#### 应用

- **自然语言处理（NLP）**：如语言翻译、情感分析、语音识别、图像字幕、文本生成等任务。
- **时间序列预测**：根据过去的时间序列数据，如每日洪水、潮汐和气象数据等，预测未来的值。
- **其他领域**：还可应用于处理传感器数据以检测短时间内的异常、音乐生成、情绪分类等场景。



尽管 RNN 在人工智能中的使用有所减少，尤其是随着转换器模型的兴起，但它在一些特定环境中仍有应用，特别是在其序列性质和记忆机制能发挥作用的场景中2。



### 实践
