本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------

### 背景

最近接了一个需求（类似财务统计数据等），要提供一个随意组合多个条件来查询订单数据的功能，看着数据库里过亿的订单量，头发不争气地又脱落了两根代表这个需求不简单。



#### 数据量分布

业财系统数据量较大表统计：

|    表名    |   行数    | 数据长度 | 索引长度 |
| :--------: | :-------: | :------: | :------: |
| 出库明细表 | 106280176 | 29.48GB  |   34GB   |
| 出库单头表 | 25344110  |   7GB    |   6GB    |
| 入库明细表 | 22766910  |   8GB    |   5GB    |
| 销售订单表 | 29578659  |   10GB   |   9GB    |
|  应收单表  | 24686267  |   5GB    |   2GB    |
|  入库单表  | 20777457  |   4GB    |   6GB    |
|  应付单表  | 15387724  |   4GB    |   2GB    |

以下是数据量较大的表数据增量趋势图，可以观察到近几个月由于新业务的增加，每月的数据增量已经达到一千万。



#### 慢查询情况

从慢查询监控平台可以看到，每天慢查询个数已经到达千量级别。慢查询不仅影响用户体验，还会大量消耗所在机器资源，严重可能导致机器宕机。

另外，MySQL数据库架构属于单机多实例，一台物理机上部署多套集群的实例，所以不仅会影响系统本身集群，还会拖累其他集群，引发雪球效应。



#### 现状

大家都知道无论是电商还是ERP系统，都存在着非常庞大的订单数据，最麻烦的是查询数据后的填充数据逻辑：

1）从数据库把上亿的订单数据查询出来。这个操作不能影响到线上业务，因此查询的订单数据库一般是从库，OK，配置多数据源来读取数据吧，而且上亿的订单一般采用的都是分库分表来存储的，我们是按照每半年一个分表。但这里也有个问题就是不同业务分表的数据量存在非常大的不平等，另外跨范围查询需要查多个分表。

2）上亿的订单数据不能一次性全部读取到内存吧，不然内存冒烟都存不下啊。所以得考虑分页，分页直接limit也不好，随着数据量越大，速度越慢，所以得考虑一个**游标**，嗯，选一个字段当游标吧，游标最好唯一且递增。

3）从多个服务获取数据，这些数据所在的服务一般都属于公司的其它部门，读取数据的时候也不能影响到人家的服务吧，你这里查询得是嘎嘎猛，一看人家的服务都崩了，这个黑锅就飞来了。所以这里得考虑限流吧，得考虑隔离吧？不说全链路隔离，成本太高，起码关键服务得**隔离**一下。

4）数据同步一段时间，产品来问，同步多久了啊，大概还有多久能完成啊，数据量大概是多少啊，一脸懵，不知道啊。



此时有人会想到采取关系型数据库+ES数据异构方式，即将数据同步到ES索引的过程。

首先需要从订单数据库查询所有的订单数据，然后根据订单数据上保存的用户ID，商品ID等信息从用户服务，商品服务查询相关信息，经过处理与组装后落到ES集群中。

之所以要查询用户信息和商品信息，是因为异构在ES索引中的订单数据，并不会与mysql中的数据一一对应，有很多根据商品类目，用户信息等查询订单信息的诉求存在，因此在这里就需要查询很多的上游服务来组装信息。

它这里有个核心的逻辑：关键信息放入到ES中，然后在非关键信息放到原来的关系型数据库。

但大家都知道，引入一个组件就意味着给系统带来了系统复杂度以及维护性。



### 要求

1、解决数据量问题

在未来五年，不用考虑数据库数据量问题，能够轻松应对未来的业务增长和覆盖公司全量业务，且具备良好的扩展性，最终可以稳定向外输出更多数据报表等。



2、解决读写性能

通过此次优化，提升报表查询效率，减少定时任务执行时间，避免因为慢查询导致任务失败和接口超时问题，提高服务稳定性。



### 方案选择

#### db存储方案选型

为解决底层表数据量问题，我们对比了以下四个方案：



##### 方案一：分库分表

1）优点

- 将数据分散到多个数据库和表中，从而减轻单一数据库的负载压力。这样可以提高数据库的读写性能和响应速度，降低查询延迟。
- 拆分的表结构相同，程序改造较少。

2）缺点

- 需要提前规划好分片规则，一旦定好规则就难以移动，扩展性比较差。
- 拆分规则很难抽象出来。
- 跨库事务问题。

3）适用场景

- 数据库面临高并发访问的压力，又需要面对海量数据的存储问题，这时需要对数据库既采用分表策略，又采用分库策略，以便同时扩展系统的并发处理能力，以及提升单表的查询性能。
- 数据有统一的业务规则主键，使数据可以均匀分布。

4）系统适用分析

- 业财系统作为底层系统，接受了各个业务系统的数据，数据比较多样性和复杂性，很难定义出一个业务主键，数据分布均匀困难。
- 若某业务数据量迅速增长或接入其他业务数据，那么可能又会面对数据量问题。



##### 方案二：冷热库

1）优点

- 将不常访问的数据从在线存储中移动到归档存储中，减少了在线存储的容量需求，从而降低了存储成本。
- 减少了在线存储中数据的数量，因此可以提高数据库读写性能。
- 可以将历史数据长期保存，避免了数据的丢失。
- 可以将数据备份到不同的存储位置，以便在需要时进行数据恢复。

2）缺点

- 需要保证归档事务性，防止归档数据同时出现在冷热库，出现数据重复。
- 需要考虑合适的归档策略，不影响服务访问。
- 需要有明确的业务边界，业务复杂的数据不适用。

3）适用场景

- 数据库中存在大量的历史数据，且查询频率比较低。
- 数据库的写入操作比读取操作更频繁。
- 数据库的存储成本较高，需要降低成本。

4）系统适用分析

- 业财系统业务数据复杂，现阶段还会更改和查询历史数据，时间口径不统一，边界比较模糊，无法确认一个准确的边界。
- 考虑后续接入更多的业务数据，由于目前无法统一数据格式，那么可能就需要重新考虑边界等问题。



##### 方案三：TiDB

1）优点

- 高度兼容 MySQL：大多数情况下，无需修改代码即可从MySQL轻松迁移至TiDB。
- 水平弹性扩展：通过简单地增加新节点即可实现 TiDB 的水平扩展，按需扩展吞吐或存储，轻松应对高并发、海量数据场景。

2）缺点

- 仍有一些MySQL的特性和行为，TiDB目前暂时不支持或表现与MySQL有差异。
- 系统复杂，组件太多。

3）适用场景

- 对数据一致性及高可靠、系统高可用、可扩展性、容灾要求较高的金融行业属性的场景。
- 对存储容量、可扩展性、并发要求较高的大量数据及高并发的OLTP场景。
- 数据汇聚、二次加工处理的场景。

4）系统适用分析

- 由于TiDB兼容了MySQL，所以改动点也较少。
- 近几年是不用考虑数据量问题，可以接入更多样化数据。
- TiDB能够支持大表经常有加列减列的需求，可扩展性高，目前也比较符合业财现状。



##### 方案四：OceanBase

1）优点

- 高性能：采用了读写分离的架构，把数据分为基线数据和增量数据。其中增量数据放在内存里（MemTable），基线数据放在SSD盘（SSTable）。对数据的修改都是增量数据，只写内存。所以DML是完全的内存操作，性能非常高。
- 高兼容：兼容常用MySQL/ORACLE功能及MySQL/ORACLE前后台协议，业务零修改或少量修改即可从MySQL/ORACLE迁移至OceanBase。
- 高可用：数据采用多副本存储，少数副本故障不影响数据可用性。

2）缺点

- 对环境要求极高，需要采购使用其指定的服务器。
- 学习和运维成本比较高。
- 尽管OceanBase具有高可用性的特性，但其实现仍然依赖于底层硬件和网络的稳定性。

3）适用场景

- 金融级数据可靠性需求。金融环境下通常对数据可靠性有更高的要求，OceanBase 每一次事务提交，对应日志总是会在多个数据中心实时同步，并持久化。
- 数据库面对飞速增长的业务数据量。

4）系统适用分析

- 目前运维没有维护，所以就不考虑此方案，大家可以参考此方案是否适用于本身系统。
- 综合以上各个方案的分析，目前最适用于转转业财系统的方案是TiDB。该方案能够在短时间内解决数据量问题，并且改动成本相对较低。



其实还有比较多的组合方法，如模块分库+分表+读写分离



#### 慢查询优化方案

在分析了慢查询语句以后，发现大部分慢查询都是由于联表查询导致的，所以此次主要解决联表问题。联表解决方案对比如下，根据适用分析选择ES方案。

| 方案 |                         业财适用分析                         |
| :--: | :----------------------------------------------------------: |
| 宽表 | 1）宽表可能包含大量重复数据，导致存储空间的浪费。这会增加数据库的存储需求，尤其在大规模数据集上会更为显著 <br />2）由于涉及到大量列和关联数据，后续性能优化可能需要考虑更多的因素，而且可能需要采用复杂的索引策略 <br />3）复杂度增加，改动量比较大 |
|  ES  | 1）通过建立索引方式解决联表问题，也一并提高了查询效率 <br />2）后续可扩展性比较高，增加查询条件等，都易实现 <br />3）需要保持数据源与ES数据一致问题 <br />4）可以减低现有的数据库索引数据量 |



### 方案实践

#### 方案实践总步骤

对于切换数据源的操作这里一般分为3个大步骤：代码双写数据+旧数据全量同步、读代码切换新数据源、代码全切换新数据源。

根据方案选择分析，最适合系统当前状况的方案是首先切换底层数据存储，然后再接入ES。

在实施这两个方案之前，我们需要考虑它们的先后顺序，并分析业财系统的现状。由于数据量的突增，考虑到现有业务和后续新增业务，同时在不影响现有使用的前提下，首要需要解决的问题是数据量。

因此，我们建议首先切换底层数据存储。这样做的好处是，即使在后续的实施中遇到问题，我们仍然可以回滚到原有的数据存储。这样既可以保证数据的完整性，也减少了实施过程中的风险。

另一方面，如果我们选择先接入ES，就需要考虑如何保证数据切换过程中的数据完整性，并且同步方式也需要考虑两种不同数据存储方案之间的兼容性，这将增加许多额外的工作量和风险。



#### 切换底层数据存储步骤

在选择数据迁移方式时，考虑到ERP系统对实时性要求并不是很高，且评估了下目前大部分数据接入写入方式，是可以接受停写几分钟，这样便大大降低了整个数据迁移成本。

迁移过程要求：

- 检查TiDB是否都能兼容目前服务中的SQL语句，保证迁移之后系统不会报错。
- 数据需要保证完整性，迁移之后需要保证MySQL库和TiDB库的数据是严格一致。
- 迁移过程中需要做到可以回滚，一旦迁移过程中出现问题，可以立即回滚到MySQL库，不会对系统可用性造成影响。



#### 接入ES

根据报表查询页面的功能和联表SQL分析，我们进行了索引模型设计，核心是优化查询性能和提高系统的响应速度。



在建立索引模型之后，我们需要考虑数据库（DB）与Elasticsearch（ES）之间增量数据的同步方式。



以下表格是对比了四种不同的同步方式，我们根据已设计的索引分析，考虑到每个索引涉及的表较多、相关业务代码尚未收口以及对实时性较高的需求，我们决定采用数据订阅的方式进行同步。在当前公司提供的实现方式中，我们选择了Kafka。



| 同步方式 |                             优点                             |                             缺点                             |
| :------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| 同步双写 |                  这种方式简单粗暴，实时性高                  | 1）业务耦合：这种方式代码侵入性强，耦合大量数据同步代码，要在写DB的地方写ES的代码 <br />2）影响性能：写入两个存储，响应时间变长，系统的性能必然会下降 <br />3）不便扩展：搜索可能有一些个性化需求，需要对数据进行聚合，这种方式不便实现 <br />4）高风险：存在双写失败丢数据风险 |
| 异步双写 | 1）性能高 <br />2）不易出现数据丢失问题 <br />3）多源写入之间相互隔离，便于扩展更多的数据源写入 | 1）硬编码问题，接入新的数据源需要实现新的消费者代码 2）系统复杂度增加，引入了消息中间件 <br />3）MQ是异步消费模型，用户写入的数据不一定可以马上看到，造成延时 |
| 定期同步 |                         实现比较简单                         |          1）实时性难以保证 <br />2）对存储压力较大           |
| 数据订阅 |            1）业务入侵较少 <br />2）实时性比较高             |             需要选型数据订阅框架，系统复杂度增加             |

在增量数据同步以后，最后一步就是需要完成历史数据的同步。

异构数据源同步有比较多的开源框架供选择，但数据同步的难点在于数据一致性的问题即数据集合A与数据集合B的完全相等问题。







参考资料：



https://mp.weixin.qq.com/s/lZN1jvNFUAIMDI3MxsDj2Q

https://mp.weixin.qq.com/s/Py_TfykL8HJcpUFLO7WEZg
