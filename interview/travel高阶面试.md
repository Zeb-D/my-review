本文章来源于：<https://github.com/Zeb-D/my-review> ，请star 强力支持，你的支持，就是我的动力。

[TOC]

------



# 业务问题（业务理解与解决方案）

## 业务场景分析

### 问题1：如何设计一个高并发的机票搜索系统？

#### 核心挑战

- 高并发查询（如促销期间瞬时流量激增）。
- 低延迟响应（用户期望实时结果）。
- 数据异构（不同航空公司的API返回格式不同）。

#### 设计方案

**分层架构**

- **客户端层**：
  - 移动端/Web端缓存静态数据（如航空公司Logo）。
  - 请求合并（如用户连续输入时防抖）。
- **API网关层**：
  - 限流（令牌桶算法）+ 熔断（Hystrix/Sentinel）。
  - 路由请求到不同服务（如国内/国际机票分离）。
- **搜索服务层**：
  - **缓存策略**：
    - 多级缓存：本地缓存（Caffeine） + 分布式缓存（Redis）。
    - 缓存预热（提前加载热门航线数据）。
  - **异步聚合**：
    - 使用CompletableFuture并行调用航空公司API，超时控制（如50%结果先返回）。
  - **索引优化**：
    - Elasticsearch集群：按航线、日期分片，N-gram分词支持模糊搜索。
- **数据层**：
  - 主从分离 + 读写分离（MySQL）。
  - 航班数据预聚合（如每天凌晨计算热门航线价格区间）。

**关键细节**

- **一致性**：缓存与数据库的更新通过消息队列（Kafka）异步同步。
- **容灾**：降级方案（如无实时数据时返回缓存的历史最低价）。



### 问题2：酒店库存系统如何避免超卖？

#### 核心挑战

- 库存扣减的原子性（如多人同时预订同一房间）。
- 分布式系统的事务一致性。

#### 设计方案

**预扣库存（Hold Inventory）**

- 用户下单时先预占库存（状态为`锁定`），支付成功后转为`已售`，超时未支付则释放。

- **实现方式**：

  - **数据库层**：

    ```
    UPDATE inventory SET stock = stock - 1  
    WHERE hotel_id = ? AND date = ? AND stock > 0;  
    ```

    - 通过乐观锁（version字段）或悲观锁（SELECT FOR UPDATE）防冲突。

  - **分布式锁**：

    - Redis + Lua脚本保证原子性：

      ```
      if redis.call("GET", key) >= quantity then  
        return redis.call("DECRBY", key, quantity)  
      else  
        return -1  
      end  
      ```



**最终一致性**

- **支付成功后**：通过Saga模式异步更新库存（若支付失败则补偿释放）。
- **消息队列**：订单服务与库存服务通过Kafka解耦，确保至少一次消费。

**防重设计**

- 订单唯一ID（雪花算法） + 幂等接口（如Redis记录已处理请求ID）。



### 问题3：如何动态定价（Dynamic Pricing）旅游套餐？

#### 核心挑战

- 实时计算（基于供需、用户画像等）。
- 策略可配置（运营人员灵活调整规则）。

#### 设计方案

1. **数据输入**

   - **实时数据**：当前库存、搜索量、竞争对手价格（爬虫）。
   - **历史数据**：季节性波动、用户购买习惯。
   - **用户画像**：VIP用户、新用户折扣。

2. **定价引擎**

   - **规则引擎**（如Drools）：

     ```
     rule "HighDemandRule"  
       when  
         $item : TravelPackage(demand > 80%)  
       then  
         $item.setPrice($item.getPrice() * 1.2);  
     end  
     ```

   - **机器学习模型**（可选）：

     - 使用时间序列预测（如Prophet）或强化学习调整参数。

3. **系统实现**

   - **服务分层**：
     - 价格计算服务（无状态，快速扩缩容）。
     - 价格缓存（Redis，TTL=10秒避免频繁计算）。
   - **动态更新**：
     - 监听库存变化（如Kafka事件触发重新计价）。



## 性能与用户体验

### 问题1：如何优化移动端API的响应时间？

#### 1. 分层优化思路

**目标**：将API响应时间控制在200ms以内（旅游行业常用标准）。
**优化方向**：

- **网络层**：
  - 使用HTTP/2或QUIC协议减少连接开销。
  - 启用CDN加速静态资源（如酒店图片、城市列表）。
  - 采用DNS预解析+IP长连接（如OkHttp的连接池）。
- **服务层**：
  - **缓存策略**：
    - 多级缓存：本地缓存（Caffeine）+ 分布式缓存（Redis，热点数据用`hot key`设计）。
    - 对机票/酒店等低频变更数据，采用`Cache-Aside`模式，TTL+主动更新结合。
  - **异步化**：
    - 非核心逻辑（如用户行为日志）通过消息队列（Kafka）异步处理。
    - 使用CompletableFuture实现并行调用（如同时查机票和酒店）。
  - **数据聚合**：
    - 避免N+1查询，用GraphQL或BFF层（Backend for Frontend）按需聚合数据。
- **数据层**：
  - 数据库优化：索引覆盖查询、读写分离（如MySQL主从+分库分表）。
  - 对复杂查询（如多维度的酒店筛选）用Elasticsearch倒排索引。

#### 2. 监控与迭代

- 全链路监控：APM工具（如SkyWalking）定位慢请求。
- A/B测试：对比优化前后的用户转化率。

Traveloka场景补充：

- 机票搜索API可预缓存热门航线（如雅加达-巴厘岛）。
- 移动端优先返回最小数据集合（如只展示价格和基础信息，详情页懒加载）。



### 问题2：如何实现个性化推荐（比如酒店推荐）？

#### 1. 推荐系统架构

**核心模块**：

- **数据收集**：
  - 用户显式数据（历史订单、收藏）。
  - 隐式数据（点击流、停留时间，通过埋点+实时日志收集）。
- **特征工程**：
  - 用户特征（偏好价格区间、地理位置）。
  - 物品特征（酒店标签：商务/度假、星级）。
  - 上下文特征（季节、当前地理位置）。
- **算法层**：
  - 协同过滤（User-Based/Item-Based）：适合冷启动后场景。
  - 深度学习（如Wide & Deep模型，平衡记忆和泛化）。
  - 实时推荐：用Flink处理实时行为事件更新推荐结果。
- **服务层**：
  - 离线训练（TensorFlow/PyTorch）+ 在线推理（TensorFlow Serving）。
  - 结果缓存（Redis ZSET存储用户个性化排序结果）。

#### 2. 工程落地

- **性能优化**：
  - 离线推荐结果预计算，在线阶段仅做轻量级排序。
  - 降级策略：当实时系统超时，返回基于热销的兜底推荐。
- **Traveloka场景补充**：
  - 结合旅游特性：推荐“航班+酒店”捆绑套餐（关联规则挖掘）。
  - 解释性：告诉用户“推荐理由”（如“因为你喜欢海景房”）。



### 问题3：如何处理高并发下的支付失败问题？

#### 1. 支付流程的容错设计

**核心挑战**：支付成功率直接影响营收，需保证最终一致性。
**分层解决方案**：

- **前端优化**：
  - 乐观锁：用户提交支付前，本地缓存库存/价格（避免中途变更）。
  - 按钮防重+倒计时自动重试（如3秒后提示“重新尝试”）。
- **后端设计**：
  - **幂等性**：
    - 支付请求唯一ID（如`orderId+timestamp`），数据库唯一索引防重复。
    - 状态机控制（如订单状态：`pending` → `paid`/`failed`）。
  - **异步补偿**：
    - 支付网关超时后，通过定时任务+人工对账修复状态。
    - 用Saga模式拆分支付流程（如先扣款再通知酒店，失败则逆向操作）。
  - **熔断与降级**：
    - 监控第三方支付API（如Xendit、Doku），失败率超阈值时切换备用通道。
    - 降级方案：允许用户“锁定价格”，稍后完成支付（如30分钟内）。
- **数据一致性**：
  - 本地事务+消息队列（如RocketMQ事务消息）确保扣款和订单状态同步。
  - 对账系统：每日核对支付网关和内部订单流水。



#### 2. Traveloka场景补充

- **高并发设计**：
  - 支付请求队列削峰（如Kafka分区按用户ID哈希，避免同一用户重复支付）。
  - 热点账户（如热门航空公司）采用Redis缓存余额+数据库异步更新。
- **国际支付**：
  - 多币种处理：实时汇率服务+边缘计算（如AWS Lambda@Edge）。





1. **结构化思维**：分层次（前端/后端/数据）、分场景（正常流程/异常流程）展开。
2. **技术深度**：提到Saga、Wide & Deep模型、HTTP/2等高级概念，同时解释落地细节。
3. **业务结合**：紧扣旅游行业特性（库存、捆绑销售、国际支付）。
4. **鲁棒性**：始终考虑降级、监控、容灾。



## 第三方集成

### 问题1：如何设计航空公司/酒店API的聚合层？

**1. 需求分析**

Traveloka需要整合多个航空公司/酒店的API（如Sabre、Amadeus、Expedia、Booking.com等），并提供统一的搜索、比价、预订接口给前端。挑战包括：

- **数据异构性**：不同供应商的API返回格式不同（XML/JSON，字段命名不同）。
- **性能优化**：聚合多个API可能导致响应时间变长。
- **高可用性**：避免因某个供应商API故障影响整体服务。

### 2. 架构设计

采用 **API聚合层（API Gateway + 聚合服务）**，核心组件如下：

#### (1) 客户端请求入口（API Gateway）

- **功能**：路由请求、认证、限流、缓存。
- **技术选型**：
  - **Kong / Nginx**（反向代理 + 负载均衡）
  - **Spring Cloud Gateway**（动态路由、熔断）

#### (2) 聚合服务（Aggregator Service）

- **职责**：
  - 并行调用多个供应商API（如航班搜索、酒店库存）。
  - 数据转换（标准化为Traveloka的DTO）。
  - 合并结果（如按价格排序）。
- **技术实现**：
  - **Java + Spring WebFlux**（非阻塞IO，提高并发能力）。
  - **CompletableFuture / Reactor**（异步并行调用）。
  - **缓存**（Redis缓存热门航线/酒店数据）。

#### (3) 数据标准化层（Adapter Pattern）

- 每个供应商API对应一个 **Adapter**，负责：
  - 请求参数转换（如Traveloka的搜索条件 → 供应商的查询格式）。
  - 响应数据标准化（如不同供应商的“价格”字段 → `price`）。

#### (4) 结果合并与排序

- 使用 **策略模式（Strategy Pattern）** 支持不同排序规则（价格、评分、距离等）。
- 去重（如相同航班可能来自不同供应商）。

#### (5) 缓存优化

- **本地缓存（Caffeine）**：缓存短期不变的数据（如机场列表）。
- **分布式缓存（Redis）**：缓存热门搜索结果（TTL + LRU策略）。

### 3. 性能优化

- **并行调用**：使用异步非阻塞IO（如WebClient + Reactor）减少延迟。
- **超时控制**：为每个供应商API设置超时（如500ms），避免慢请求拖累整体响应。
- **分页加载**：先返回部分数据，再异步加载剩余（如“加载更多”）。

### 4. 高可用设计

- **熔断降级**（如Hystrix / Resilience4j）：如果某供应商API失败，自动降级（如返回缓存数据或默认推荐）。
- **重试机制**：对可重试错误（如HTTP 503）进行指数退避重试。



## 问题2：如何保证第三方API的可靠性和熔断？

**1. 可靠性挑战**

- 第三方API可能不稳定（超时、限流、宕机）。
- 需要保证Traveloka的核心功能（如搜索、预订）不受影响。

### 2. 解决方案

#### **(1) 熔断机制（Circuit Breaker）**

- **目的**：防止持续调用故障API导致系统雪崩。
- **实现**：
  - **Hystrix / Resilience4j**：当错误率超过阈值（如50%），熔断并快速失败。
  - **熔断状态**：
    - **Closed**：正常请求。
    - **Open**：直接拒绝请求，走降级逻辑。
    - **Half-Open**：尝试少量请求，检测是否恢复。

#### **(2) 降级策略（Fallback）**

- **静态降级**：返回默认数据（如上次缓存的价格）。
- **动态降级**：
  - 使用 **备用供应商**（如主用Amadeus失败，切换Sabre）。
  - 返回 **简化数据**（如只显示基本航班信息，不返回座位详情）。

#### **(3) 限流（Rate Limiting）**

- **目的**：避免Traveloka的调用超过供应商的配额。
- **实现**：
  - **Redis + Token Bucket**：控制QPS（如1000次/分钟）。
  - **队列缓冲**：超出限制的请求进入队列，异步处理。

#### **(4) 监控与告警**

- **Prometheus + Grafana**：监控API成功率、延迟、熔断状态。
- **告警规则**：
  - 错误率 > 5% → 触发Slack/邮件告警。
  - 平均延迟 > 1s → 通知运维团队。

#### **(5) 重试策略（Retry Policy）**

- **场景**：适用于临时性错误（如网络抖动）。
- **策略**：
  - **指数退避**：第一次重试1s后，第二次2s，第三次4s。
  - **限制重试次数**（如最多3次）。



# 系统设计问题

## 基础架构设计

### 问题1：设计一个支持高并发的机票预订系统（分布式锁、一致性、DB分片）

#### 核心挑战

1. **高并发抢购**：避免超卖（如同一航班最后一个座位被重复预订）。
2. **数据一致性**：订单创建与库存扣减必须原子性。
3. **数据库扩展**：单机数据库无法承受峰值流量。

#### **设计方案**

**1. 分层架构**

```
客户端 → CDN/静态资源 → API网关 → 微服务集群（订单/库存/支付） → 分库分表DB → 缓存/消息队列  
```

**2. 关键实现**

- **分布式锁**：

  - 使用 **Redis + RedLock**（或Zookeeper）实现锁，防止并发扣减库存。

  - 伪代码：

    ```
    // 加锁（设置过期时间防止死锁）  
    boolean lock = redis.set("flight:123:lock", requestId, "NX", "EX", 10);  
    if (lock) {  
        try {  
            // 检查库存 → 扣减 → 创建订单  
        } finally {  
            // 释放锁（Lua脚本保证原子性）  
            redis.eval("if redis.call('get',KEYS[1])==ARGV[1] then return redis.call('del',KEYS[1]) end",  
                      Collections.singletonList("flight:123:lock"), requestId);  
        }  
    }  
    ```

- **最终一致性**：

  - 采用 **Saga模式** + **MQ（如Kafka）**：
    - 步骤1：订单服务创建订单（状态为"Pending"），发送扣减库存事件。
    - 步骤2：库存服务扣减库存，成功后发送确认事件。
    - 步骤3：订单服务更新订单为"Confirmed"。
  - **补偿机制**：若库存扣减失败，通过MQ触发订单取消和退款。

- **数据库分片**：

  - **垂直分片**：订单表与航班表分离（不同数据库实例）。
  - **水平分片**：按航班号哈希分片（如`订单表_分片1`到`订单表_分片N`）。
  - **读写分离**：主库写，从库读（通过ShardingSphere或MyCat实现）。



### 问题2：如何设计缓存策略（如Redis）缓解数据库压力？

#### 核心策略

**1. 多级缓存**

- **本地缓存（Caffeine）**：缓存静态数据（如城市列表），TTL=5分钟。
- **分布式缓存（Redis）**：缓存动态数据（如航班余票），TTL=1分钟 + 异步刷新。

**2. 缓存模式**

- **Cache-Aside**：

  ```
  public Flight getFlight(String flightId) {  
      // 1. 先查缓存  
      Flight flight = redis.get("flight:" + flightId);  
      if (flight == null) {  
          // 2. 查数据库  
          flight = db.query("SELECT * FROM flights WHERE id = ?", flightId);  
          // 3. 回填缓存（设置短暂TTL防脏数据）  
          redis.setex("flight:" + flightId, 60, flight);  
      }  
      return flight;  
  }  
  ```

- **Write-Behind**（高阶）：通过消息队列异步更新缓存（适合写多读少场景）。

**3. 防雪崩/击穿/穿透**

- **雪崩**：缓存Key设置随机TTL（如60±10秒）。
- **击穿**：用Redis的`SETNX`实现互斥锁重建缓存。
- **穿透**：布隆过滤器（Bloom Filter）拦截无效查询（如不存在的航班ID）。





### 问题3：如何实现分布式会话管理（用户登录状态）？

#### **方案对比与选型**

| 方案               | 优点               | 缺点                        | 适用场景           |
| :----------------- | :----------------- | :-------------------------- | :----------------- |
| **Session复制**    | 无单点问题         | 网络带宽消耗大              | 小型集群（不推荐） |
| **Sticky Session** | 简单高效           | 负载不均，扩容困难          | 短期活动场景       |
| **Redis集中存储**  | 扩展性强，一致性高 | Redis单点故障风险（需集群） | **Traveloka推荐**  |

#### **Redis实现细节**

1. **登录流程**：

   - 用户登录后生成`Token`（JWT），Redis存储`Token:UserID`映射，TTL=30分钟。

   - 伪代码：

     ```
     String token = UUID.randomUUID().toString();  
     redis.setex("session:" + token, 1800, userId);  
     response.setCookie("SESSION_TOKEN", token);  
     ```

2. **会话校验**：

   - 网关层（如Spring Cloud Gateway）拦截请求，从Redis校验Token有效性。

3. **高可用优化**：

   - Redis Cluster + 哨兵模式（自动故障转移）。
   - 本地缓存Token（降低Redis查询压力，TTL更短）。



## **数据一致性**

- *如何保证订单创建和库存扣减的最终一致性？（Saga、TCC、消息队列）*
- *支付成功后，如何通知多个子系统（订单、库存、积分）？*



## 高可用与容灾

### 问题1：如何保证订单创建和库存扣减的最终一致性？（Saga、TCC、消息队列）

#### **1. 问题背景**

在在线旅游平台（如Traveloka）中，用户下单后需要同时完成**订单创建**和**库存扣减**（如酒店房间、机票座位）。这两个操作必须保证一致性，否则会出现超卖（库存未扣减但订单创建成功）或丢单（库存扣减但订单失败）的问题。

#### **2. 解决方案对比**

以下是几种常见的分布式事务方案：

| 方案          | 原理                                                         | 适用场景                 | 优缺点                                            |
| :------------ | :----------------------------------------------------------- | :----------------------- | :------------------------------------------------ |
| **2PC（XA）** | 通过协调者（Coordinator）管理参与者（DB），分两阶段提交/回滚。 | 传统数据库，强一致性要求 | ✅ 强一致性 ❌ 性能差，阻塞严重，不适合高并发       |
| **TCC**       | Try-Confirm-Cancel 三阶段：预留资源 -> 确认/取消。           | 高并发，需灵活控制资源   | ✅ 高性能，最终一致 ❌ 业务侵入性强，需实现补偿逻辑 |
| **Saga**      | 长事务拆分为多个本地事务，失败时触发补偿事务。               | 长流程业务（如旅游订单） | ✅ 适合复杂业务流程 ❌ 可能出现脏读（需业务容忍）   |
| **消息队列**  | 通过消息可靠性（如RabbitMQ/Kafka）保证操作最终一致。         | 异步场景，允许短暂延迟   | ✅ 解耦，高吞吐 ❌ 需处理消息丢失/重复（如幂等）    |

#### **3. Traveloka的推荐方案**

结合旅游行业特点（高并发、允许最终一致），建议采用 **TCC** 或 **Saga + 消息队列** 的混合模式：

- **TCC模式**（适合核心资源如机票库存）：

  1. **Try阶段**：冻结库存（如将航班座位状态改为“锁定”）。
  2. **Confirm阶段**：订单创建成功后，确认扣减库存。
  3. **Cancel阶段**：订单失败后，释放冻结的库存。

  - *优点*：避免长期占用资源，性能较高。
  - *挑战*：需实现冻结状态的查询和超时释放（如用Redis TTL）。

- **Saga + 消息队列**（适合复杂流程如酒店+机票套餐）：

  1. 订单服务创建订单，发布“订单已创建”事件到Kafka。
  2. 库存服务消费事件，扣减库存，若失败则发布“库存扣减失败”事件。
  3. 订单服务监听失败事件，触发订单状态回滚（补偿）。

  - *关键点*：
    - 消息幂等（通过唯一订单ID去重）。
    - 补偿逻辑需考虑部分成功（如库存已扣但积分未加）。

#### **4. 容错设计**

- **超时控制**：TCC的Try阶段需设置超时（如30分钟未Confirm自动Cancel）。
- **对账Job**：定时检查订单与库存状态不一致的情况（如订单成功但库存未扣）。



### 问题2：支付成功后，如何通知多个子系统（订单、库存、积分）？

#### **1. 业务需求**

支付成功后需同步更新：

- **订单状态**：从“待支付”改为“已支付”。
- **库存状态**：确认扣减（如机票座位最终占用）。
- **积分系统**：为用户增加积分。
- **通知系统**：发送短信/邮件确认。

#### **2. 解决方案**

##### **方案1：同步调用（不推荐）**

- 支付服务直接调用订单、库存、积分等服务的API。
- *问题*：
  - 耦合度高，任一服务失败会导致支付结果不一致。
  - 性能差（链式调用延迟叠加）。

##### **方案2：事件驱动（推荐）**

1. **支付服务**完成支付后，发布事件到消息队列（如Kafka）：

   ```
{
     "event_id": "123",
  "order_id": "ORD-1001",
     "user_id": "456",
  "amount": 500,
     "status": "SUCCESS"
}
   ```

2. **各子系统订阅事件并处理**：

   - **订单服务**：更新订单状态为“已支付”。
   - **库存服务**：确认库存扣减（TCC的Confirm阶段）。
   - **积分服务**：增加用户积分（需幂等，防止重复累加）。
   - **通知服务**：触发邮件/SMS推送。

3. **关键设计**：

   - **消息可靠性**：
     - Kafka支持高可用和持久化。
     - 生产者需确认BrokerACK，消费者需手动提交Offset。
   - **幂等性**：
     - 各服务通过`event_id`或`order_id`去重（如数据库唯一索引）。
   - **补偿机制**：
     - 若积分服务宕机，可通过定时任务重新处理未完成事件。

##### **方案3：事务性发件箱（Transactional Outbox）**

- 支付服务先将事件写入数据库（与支付事务同一事务），再通过CDC（如Debezium）捕获变更并发布到MQ。
- *适用场景*：需要绝对保证事件不丢失（如金融级系统）。

#### **3. Traveloka的优化建议**

- **分级通知**：
  - 关键路径（订单、库存）实时处理。
  - 非关键路径（积分、通知）可异步延迟处理。
- **降级策略**：
  - 若积分服务不可用，可先记录日志，后续补发。



## 扩展性问题

### 问题1：系统如何应对促销活动（如双十一级别的流量）？

促销活动（如“Traveloka 12.12”）会带来**瞬时高并发、流量突增**，系统需要在**架构设计、缓存策略、限流熔断、数据库优化**等方面做充分准备。

### **1. 流量预估与扩容（预防阶段）**

- **流量预测**：基于历史数据（如去年12.12的QPS、订单量）预估峰值，提前扩容。
- **弹性伸缩**：
  - **横向扩展**：使用Kubernetes/Auto Scaling Group动态扩容应用实例。
  - **云服务弹性**：临时提升数据库/缓存规格（如AWS RDS读写分离、Redis Cluster）。

### **2. 前端优化（减少后端压力）**

- **静态资源CDN**：JS/CSS/图片缓存到CDN（如Cloudflare）。
- **客户端限流**：按钮防重复点击（Debounce）、排队动画（如“当前排队1万人”）。
- **降级策略**：非核心功能（如用户评论）可暂时关闭。

### **3. 缓存策略（抗住读高并发）**

- **多级缓存**：
  - **浏览器缓存**：HTTP Cache-Control。
  - **CDN缓存**：静态化热门旅游套餐页面。
  - **Redis集群**：
    - 热点数据预加载（如首页推荐酒店）。
    - 使用分布式锁防止缓存击穿（如`Redisson`）。
- **缓存一致性**：
  - 异步更新（Binlog监听 + Canal/MQ）。

### **4. 异步化 & 消息队列（削峰填谷）**

- **订单异步化**：用户提交订单后，先返回“排队中”，后端通过MQ（如Kafka）异步处理。
- **支付最终一致性**：支付成功后，通过MQ通知库存、积分等子系统。

### **5. 数据库优化（写高并发）**

- **读写分离**：主库写，从库读（如MySQL主从+ProxySQL）。
- **分库分表**：订单表按`user_id`分片（见问题2）。
- **SQL优化**：避免慢查询（索引、批量插入）。

### **6. 限流 & 熔断（保护系统）**

- **API限流**：
  - Nginx层限流（`limit_req`）。
  - 分布式限流（Redis + Lua）。
- **熔断降级**：
  - 第三方API不可用时，启用本地缓存（如Hystrix）。

### **7. 监控 & 应急**

- **实时监控**：Prometheus + Grafana监控QPS、DB负载。
- **预案演练**：提前模拟流量（如压测工具JMeter）。



### 问题2：数据库分库分表策略如何设计？

### **1. 何时需要分库分表？**

- 单表数据量 > 千万级，SQL性能下降。
- 写入QPS超过单库承受能力（如MySQL单机5k TPS）。

### **2. 分片键选择（最关键）**

- **订单表**：
  - 按`user_id`分片（保证同一用户的订单在一个分片）。
  - 按`order_id`哈希分片（数据均匀分布）。
- **酒店库存表**：
  - 按`hotel_id`分片（避免跨分片扣减库存）。

### **3. 分片策略**

- **范围分片**（Range）：
  - 例如按`create_time`分表（每月一张表）。
  - **优点**：易于扩容。
  - **缺点**：可能热点数据（如最新月份）。
- **哈希分片**（Hash）：
  - 例如`order_id % 16`分散到16个库。
  - **优点**：数据均匀。
  - **缺点**：扩容需迁移数据（一致性哈希可缓解）。

### **4. 分库分表中间件**

- **ShardingSphere**（推荐）：SQL透明路由。
- **MyCat**：老牌代理层。

### **5. 分库分表后的挑战**

- **跨分片查询**：
  - 全局表（如城市列表）冗余全库。
  - 分页查询：先查ES聚合再回表。
- **分布式事务**：
  - 订单创建 + 库存扣减 → Saga模式。
- **ID生成**：
  - 雪花算法（Snowflake）避免主键冲突。

### **6. 扩容方案**

- **双写迁移**：新旧库同时写入，逐步迁移。
